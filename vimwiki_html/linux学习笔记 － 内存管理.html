<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<link rel="Stylesheet" type="text/css" href="style.css">
<title>linux学习笔记 － 内存管理</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>
<body>

<h1>目录</h1>
<div class="toc">
<ul>
<li><a href="#toc_0.1">kernel内存映射</a>
<ul>
<li><a href="#toc_0.1.1">Kernel Virtual Address Mapping</a>
<li><a href="#toc_0.1.2">Decompressor Symbols</a>
<li><a href="#toc_0.1.3">Kernel Symbols</a>
</ul>
<li><a href="#toc_0.2">内存组织</a>
<ul>
<li><a href="#toc_0.2.1">内存管理架构图</a>
<li><a href="#toc_0.2.2">node-zone-page组织关系</a>
<li><a href="#toc_0.2.3">内存分配关系</a>
<li><a href="#toc_0.2.4">buddy组织图</a>
<li><a href="#toc_0.2.5">swap水线</a>
</ul>
<li><a href="#toc_0.3">SLUB分配器</a>
<ul>
<li><a href="#toc_0.3.1">模型</a>
<li><a href="#toc_0.3.2">核心数据结构关系</a>
</ul>
<li><a href="#toc_0.4">页表机制</a>
<ul>
<li><a href="#toc_0.4.1">页表通用框架</a>
<li><a href="#toc_0.4.2">single-step page table walk</a>
<li><a href="#toc_0.4.3">two-step page table walk</a>
<li><a href="#toc_0.4.4">内核页表布局全图</a>
</ul>
<li><a href="#toc_0.5">代码阅读</a>
<ul>
<li><a href="#toc_0.5.1">kmalloc/kfree</a>
<ul>
<li><a href="#toc_0.5.1.1">kmalloc</a>
<li><a href="#toc_0.5.1.2">kfree</a>
</ul>
<li><a href="#toc_0.5.2">kmem_cache_create</a>
<li><a href="#toc_0.5.3">数据结构</a>
<ul>
<li><a href="#toc_0.5.3.1">pg_data_t</a>
<li><a href="#toc_0.5.3.2">zone</a>
<li><a href="#toc_0.5.3.3">page</a>
<li><a href="#toc_0.5.3.4">pageflags</a>
<li><a href="#toc_0.5.3.5">kmem_cache</a>
<li><a href="#toc_0.5.3.6">kmem_cache_cpu</a>
<li><a href="#toc_0.5.3.7">kmem_cache_node</a>
<li><a href="#toc_0.5.3.8">vmap_area/vmap_area_list</a>
<li><a href="#toc_0.5.3.9">vm_struct</a>
</ul>
</ul>
<li><a href="#toc_0.6">调试经验</a>
<ul>
<li><a href="#toc_0.6.1">全局变量</a>
<li><a href="#toc_0.6.2">文件节点</a>
<li><a href="#toc_0.6.3">内存泄漏检测</a>
<li><a href="#toc_0.6.4">内存越界检测</a>
<li><a href="#toc_0.6.5">Native 的内存泄漏和越界</a>
<li><a href="#toc_0.6.6">根据已分配内存地址找到slab</a>
<li><a href="#toc_0.6.7">找到所有partial的页</a>
<li><a href="#toc_0.6.8">查找kmem里的freelist</a>
<li><a href="#toc_0.6.9">虚拟地址转物理地址</a>
<li><a href="#toc_0.6.10">计算总内存大小</a>
</ul>
<li><a href="#toc_0.7">参考资料</a>
</ul>
</ul>
</div>

<hr />
<h2 id="toc_0.1">kernel内存映射</h2>
<h3 id="toc_0.1.1">Kernel Virtual Address Mapping</h3>
<p>
<img src="file:///home/likewise-open/SPREADTRUM/hua.fang/vimwiki/vimwiki/images/kernel_virtual_address_mapping_overview.gif" />
</p>

<table>
<tr>
<td>
粉色部分0xbf80 0000 ~ 0xc000 0000
</td>
<td>
modules及kpmap
</td>
<td>
从下面的板级宏定义我们可以看到，modules放在这段位置是因为它需要和kernel code段在32MB寻址空间内。</BR>kpmap为什么放这段空间我还不清楚，这个是在map highmem时用到的。
</td>
</tr>
<tr>
<td>
橙色部分0xc000 0000 ~ 0xe000 0000
</td>
<td>
lowmem(低端内存，即zone[Normal])
</td>
<td>
这段映射是一对一的平坦映射，也就是说kernel初始化这段映射后，页表将不会改变。</BR>这样即可以省去不断的修改页表，刷新TLB（TLB可以认为是页表的硬件cache，如果访问的虚拟地址的页表在这个cache中，则CPU无需访问DDR寻址页表了，这样可以提高IO效率）了。</BR>显然这段地址空间非常珍贵，因为这段映射的效率高。</BR>从图中我们可以看到，在512MB映射空间中，有128MB预留给PMEM（android特有的连续物理内存管理机制），16MB预留CP（modem运行空间）。</BR>实际可用lowmem大致只有360MB。
</td>
</tr>
<tr>
<td>
蓝色部分0xe000 0000 ~ 0xf000 0000
</td>
<td>
highmem（高端内存，即zone[!HighMem]）
</td>
<td>
因为示例为1GB DDR，因此需要高端内存映射部分物理地址空间。
</td>
</tr>
<tr>
<td>
绿色部分0xf000 0000 ~ 0xffc0 0000
</td>
<td>
IO映射区域
</td>
<td>
我们知道在内核空间，比如写驱动的时候，需要访问芯片的寄存器（IO空间），部分IO空间映射是通过ioremap在VMALLOC区域动态申请映射，还有部分是系统初始化时通过iotable_init静态映射的。</BR>图中我们可以看到在IO静态映射区域有大约200MB的空间没有使用。这个是不是太浪费了呢？
</td>
</tr>
<tr>
<td>
紫色部分
</td>
<td>
没什么花头，ARM default定义就是这样的。
</td>
<td>
&nbsp;
</td>
</tr>
</table>
<p>
<img src="file:///home/likewise-open/SPREADTRUM/hua.fang/vimwiki/vimwiki/images/memory_mapping_overview.gif" />
</p>

<h3 id="toc_0.1.2">Decompressor Symbols</h3>
<table>
<tr>
<td>
<strong>Macro name</strong>
</td>
<td>
<strong>description</strong>
</td>
<td>
<strong>example</strong>
</td>
</tr>
<tr>
<td>
ZTEXTADDR</BR>[arch/arm/boot/compressed/Makefile]
</td>
<td>
Start address of decompressor.</BR>There's no point in talking about virtual or physical addresses here, since the MMU will be off at the time when you call the decompressor code.</BR>You normally call the kernel at this address to start it booting.</BR>This doesn't have to be located in RAM, it can be in flash or other read-only or read-write addressable medium.
</td>
<td>
0x0</BR>ZTEXTADDR        := $(CONFIG_ZBOOT_ROM_TEXT)</BR>ONFIG_ZBOOT_ROM_TEXT=0x0
</td>
</tr>
<tr>
<td>
ZBSSADDR</BR>[arch/arm/boot/compressed/Makefile]
</td>
<td>
Start address of zero-initialised work area for the decompressor.</BR>This must be pointing at RAM.</BR>The decompressor will zero initialize this for you.</BR>Again, the MMU will be off.
</td>
<td>
0x0</BR>ZBSSADDR   := $(CONFIG_ZBOOT_ROM_BSS)</BR>CONFIG_ZBOOT_ROM_BSS=0x0
</td>
</tr>
<tr>
<td>
ZRELADDR</BR>[arch/arm/boot/Makefile]
</td>
<td>
This is the address where the decompressed kernel will be written, and eventually executed.</BR>The following constraint must be valid:</BR>!__virt_to_phys(TEXTADDR) == ZRELADDR</BR>The initial part of the kernel is carefully coded to be position independent.</BR>Note: the following conditions must always be true:</BR>ZRELADDR == virt_to_phys(PAGE_OFFSET + TEXT_OFFSET)
</td>
<td>
0x81088000</BR>ZRELADDR    := \((zreladdr-y)</BR>zreladdr-y       := \)(!_<em>ZRELADDR)</BR>!</em>_ZRELADDR = TEXT_OFFSET + 0x80000000</BR>[arch/arm/mach-pxa/Makefile.boot]
</td>
</tr>
<tr>
<td>
INITRD_PHYS
</td>
<td>
Physical address to place the initial RAM disk.</BR>Only relevant if you are using the bootpImage stuff (which only works on the old struct param_struct).</BR>INITRD_PHYS must be in RAM
</td>
<td>
Not defined
</td>
</tr>
<tr>
<td>
INITRD_VIRT
</td>
<td>
Virtual address of the initial RAM disk.</BR>The following constraint must be valid:</BR>!__virt_to_phys(INITRD_VIRT) == INITRD_PHYS
</td>
<td>
Not defined
</td>
</tr>
<tr>
<td>
PARAMS_PHYS
</td>
<td>
Physical address of the struct param_struct or tag list, giving the kernel various parameters about its execution environment.</BR>PARAMS_PHYS must be within 4MB of ZRELADDR
</td>
<td>
Not defined
</td>
</tr>
</table>

<h3 id="toc_0.1.3">Kernel Symbols</h3>
<table>
<tr>
<td>
<strong>Macro name</strong>
</td>
<td>
<strong>description</strong>
</td>
<td>
<strong>example</strong>
</td>
</tr>
<tr>
<td>
PHYS_OFFSET</BR>[arch/arm/include/asm/memory.h]
</td>
<td>
Physical start address of the first bank of RAM.
</td>
<td>
#define PHYS_OFFSET      PLAT_PHYS_OFFSET</BR>#define PLAT_PHYS_OFFSET    UL(0x80000000)</BR>[arch/arm/mach-pxa/include/mach/memory.h]
</td>
</tr>
<tr>
<td>
PAGE_OFFSET</BR>[arch/arm/include/asm/memory.h]
</td>
<td>
Virtual start address of the first bank of RAM.</BR>During the kernel boot phase, virtual address PAGE_OFFSET will be mapped to physical address PHYS_OFFSET, along with any other mappings you supply.</BR>This should be the same value as TASK_SIZE.
</td>
<td>
CONFIG_PAGE_OFFSET</BR>=0xC0000000
</td>
</tr>
<tr>
<td>
TASK_SIZE</BR>[arch/arm/include/asm/memory.h]
</td>
<td>
The maximum size of a user process in bytes.</BR>Since user space always starts at zero, this is the maximum address that a user process can access+1.</BR>The user space stack grows down from this address.</BR>Any virtual address below TASK_SIZE is deemed to be user process area, and therefore managed dynamically on a process by process basis by the kernel.</BR>I'll call this the user segment.</BR>Anything above TASK_SIZE is common to all processes.</BR>I'll call this the kernel segment.</BR> (In other words, you can't put IO mappings below TASK_SIZE, and hence PAGE_OFFSET).
</td>
<td>
CONFIG_PAGE_OFFSET</BR>-0x01000000</BR>=0xBF000000
</td>
</tr>
<tr>
<td>
TASK_UNMAPPED_BASE</BR>[arch/arm/include/asm/memory.h]
</td>
<td>
the lower boundary of the mmap VM area
</td>
<td>
CONFIG_PAGE_OFFSET/3</BR>=0x40000000
</td>
</tr>
<tr>
<td>
MODULES_VADDR</BR>[arch/arm/include/asm/memory.h]
</td>
<td>
The module space lives between the addresses given by TASK_SIZE and PAGE_OFFSET - it must be within 32MB of the kernel text.</BR>TEXT_OFFSET does not allow to use 16MB modules area as ARM32 branches to kernel may go out of range taking into account the kernel .text size
</td>
<td>
PAGE_OFFSET</BR>- 8*1024*1024</BR>=0x0XBF800000
</td>
</tr>
<tr>
<td>
MODULES_END</BR>[arch/arm/include/asm/memory.h]
</td>
<td>
The highmem pkmap virtual space shares the end of the module area.
</td>
<td>
0XBFE00000</BR>#ifdef CONFIG_HIGHMEM</BR>#define MODULES_END           (PAGE_OFFSET - PMD_SIZE)</BR>#else</BR>#define MODULES_END           (PAGE_OFFSET)</BR>#endif
</td>
</tr>
<tr>
<td>
TEXTADDR
</td>
<td>
Virtual start address of kernel, normally PAGE_OFFSET + 0x8000.</BR>This is where the kernel image ends up.</BR>With the latest kernels, it must be located at 32768 bytes into a 128MB region.</BR>Previous kernels placed a restriction of 256MB here.
</td>
<td>
&nbsp;
</td>
<td>
&nbsp;
</td>
</tr>
<tr>
<td>
DATAADDR
</td>
<td>
Virtual address for the kernel data segment.</BR>Must not be defined when using the decompressor.
</td>
<td>
&nbsp;
</td>
</tr>
<tr>
<td>
VMALLOC_START</BR>VMALLOC_END</BR>[arch/arm/mach-pxa/include/mach/vmalloc.h]
</td>
<td>
Virtual addresses bounding the vmalloc() area.</BR>There must not be any static mappings in this area; vmalloc will overwrite them.</BR>The addresses must also be in the kernel segment (see above). </BR>Normally, the vmalloc() area starts VMALLOC_OFFSET bytes above the last virtual RAM address (found using variable high_memory).
</td>
<td>
#define VMALLOC_END       (0xf0000000UL)</BR>The default vmalloc size is 128MB.</BR>vmalloc_min = (VMALLOC_END - SZ_128M);</BR>[defined in arch/arm/mm/mmu.c]</BR>If vmalloc is configured passed by OSL, then it’s redefined.</BR>early_param("vmalloc", early_vmalloc);</BR>[defined in arch/arm/mm/mmu.c]
</td>
</tr>
<tr>
<td>
VMALLOC_OFFSET</BR>[arch/arm/include/asm/pgtable.h]
</td>
<td>
Offset normally incorporated into VMALLOC_START to provide a hole between virtual RAM and the vmalloc area.</BR>We do this to allow out of bounds memory accesses (eg, something writing off the end of the mapped memory map) to be caught.</BR>Normally set to 8MB.
</td>
<td>
#define VMALLOC_OFFSET               (8*1024*1024)
</td>
</tr>
<tr>
<td>
CONSISTENT_DMA_SIZE</BR>CONSISTENT_BASE</BR>CONSISTENT_END</BR>[arch/arm/include/asm/memory.h]
</td>
<td>
Size of DMA-consistent memory region.</BR>Must be multiple of 2M, between 2MB and 14MB inclusive.
</td>
<td>
CONSISTENT_DMA_SIZE = 2MB</BR>CONSISTENT_BASE = 0XFFC00000</BR>CONSISTENT_END = 0XFFE00000
</td>
</tr>
<tr>
<td>
FIXADDR_START</BR>FIXADDR_TOP</BR>FIXADDR_SIZE</BR>[arch/arm/include/asm/fixmap.h]
</td>
<td>
fixed virtual addresses
</td>
<td>
#define FIXADDR_START          0xfff00000UL</BR>#define FIXADDR_TOP            0xfffe0000UL</BR>#define FIXADDR_SIZE           (FIXADDR_TOP - FIXADDR_START)
</td>
</tr>
<tr>
<td>
PKMAP_BASE</BR>[arch/arm/include/asm/highmen.h]
</td>
<td>
&nbsp;
</td>
<td>
0XBFE00000</BR>#define PKMAP_BASE               (PAGE_OFFSET - PMD_SIZE)
</td>
</tr>
</table>

<h2 id="toc_0.2">内存组织</h2>
<h3 id="toc_0.2.1">内存管理架构图</h3>
<p>
<img src="file:///home/likewise-open/SPREADTRUM/hua.fang/vimwiki/vimwiki/images/linux_kernel_memory_management_arch.jpg" />
</p>

<h3 id="toc_0.2.2">node-zone-page组织关系</h3>
<p>
<img src="file:///home/likewise-open/SPREADTRUM/hua.fang/vimwiki/vimwiki/images/linux_kernel_memory_node_zone_page.gif" />
</p>

<h3 id="toc_0.2.3">内存分配关系</h3>
<p>
<img src="file:///home/likewise-open/SPREADTRUM/hua.fang/vimwiki/vimwiki/images/linux_kernel_memory_level.png" />
</p>

<h3 id="toc_0.2.4">buddy组织图</h3>
<p>
<img src="file:///home/likewise-open/SPREADTRUM/hua.fang/vimwiki/vimwiki/images/linux_kernel_memory_buddy.png" />
</p>

<h3 id="toc_0.2.5">swap水线</h3>
<p>
<img src="file:///home/likewise-open/SPREADTRUM/hua.fang/vimwiki/vimwiki/images/linux_kernel_memory_swapper.jpg" />
</p>

<h2 id="toc_0.3">SLUB分配器</h2>
<h3 id="toc_0.3.1">模型</h3>
<p>
<img src="file:///home/likewise-open/SPREADTRUM/hua.fang/vimwiki/vimwiki/images/slub_1.jpg" />
</p>

<h3 id="toc_0.3.2">核心数据结构关系</h3>
<p>
<img src="file:///home/likewise-open/SPREADTRUM/hua.fang/vimwiki/vimwiki/images/slub_struct.png" />
</p>

<h2 id="toc_0.4">页表机制</h2>
<h3 id="toc_0.4.1">页表通用框架</h3>
<p>
<img src="file:///home/likewise-open/SPREADTRUM/hua.fang/vimwiki/vimwiki/images/mmu_page_table.jpg" />
</p>

<ul>
<li>
page dir entry

</ul>
<p>
<img src="file:///home/likewise-open/SPREADTRUM/hua.fang/vimwiki/vimwiki/images/mmu_page_table_l1.png" />
</p>

<h3 id="toc_0.4.2">single-step page table walk</h3>
<p>
<img src="file:///home/likewise-open/SPREADTRUM/hua.fang/vimwiki/vimwiki/images/single-tep_page_table_walk.png" />
</p>

<h3 id="toc_0.4.3">two-step page table walk</h3>
<p>
<img src="file:///home/likewise-open/SPREADTRUM/hua.fang/vimwiki/vimwiki/images/two_step_page_table_walk.png" />
</p>

<h3 id="toc_0.4.4">内核页表布局全图</h3>
<p>
<img src="file:///home/likewise-open/SPREADTRUM/hua.fang/vimwiki/vimwiki/images/pg_map.gif" />
</p>


<h2 id="toc_0.5">代码阅读</h2>
<h3 id="toc_0.5.1">kmalloc/kfree</h3>
<h4 id="toc_0.5.1.1">kmalloc</h4>
<h4 id="toc_0.5.1.2">kfree</h4>

<h3 id="toc_0.5.2">kmem_cache_create</h3>

<h3 id="toc_0.5.3">数据结构</h3>
<h4 id="toc_0.5.3.1">pg_data_t</h4>
<pre>
kernel/include/linux/mmzone.h

typedef struct pglist_data {
	struct zone node_zones[MAX_NR_ZONES];                       // 包含了结点中各内存域的数据结构
	struct zonelist node_zonelists[MAX_ZONELISTS];              // 指定备用结点及其内存域列表以便当前结点没有可用空间时在备用结点分配内存
	int nr_zones;                                               // 不同内存域的数目
#ifdef CONFIG_FLAT_NODE_MEM_MAP	/* means !SPARSEMEM */
	struct page *node_mem_map;                                  // 指向page实例数组的指针，用于描述结点的所有物理内存页，包含了结点中所有的内存域的页
#ifdef CONFIG_MEMCG
	struct page_cgroup *node_page_cgroup;
#endif
#endif
#ifndef CONFIG_NO_BOOTMEM
	struct bootmem_data *bdata;                                 // 指向自举内存分配器数据结构的实例
#endif
#ifdef CONFIG_MEMORY_HOTPLUG
	/*
	 * Must be held any time you expect node_start_pfn, node_present_pages
	 * or node_spanned_pages stay constant.  Holding this will also
	 * guarantee that any pfn_valid() stays that way.
	 *
	 * Nests above zone-&gt;lock and zone-&gt;size_seqlock.
	 */
	spinlock_t node_size_lock;
#endif
	unsigned long node_start_pfn;                               // 该NUMA结点第一个页帧的逻辑编号
	unsigned long node_present_pages;                           // 结点中页帧的数目
	unsigned long node_spanned_pages;                           // 该结点以页帧为单位计算的长度，包括空洞
	int node_id;                                                // 全局结点ID，系统中的NUMA结点都从0开始编号
	nodemask_t reclaim_nodes;	/* Nodes allowed to reclaim from */
	wait_queue_head_t kswapd_wait;                              // 交换守护进程的等待队列，在将页帧换出结点时会用到
	wait_queue_head_t pfmemalloc_wait;
	struct task_struct *kswapd;	/* Protected by lock_memory_hotplug() */
	int kswapd_max_order;                                       // 用于页交换子系统
	enum zone_type classzone_idx;
#ifdef CONFIG_NUMA_BALANCING
	/*
	 * Lock serializing the per destination node AutoNUMA memory
	 * migration rate limiting data.
	 */
	spinlock_t numabalancing_migrate_lock;

	/* Rate limiting time interval */
	unsigned long numabalancing_migrate_next_window;

	/* Number of pages migrated during the rate limiting time interval */
	unsigned long numabalancing_migrate_nr_pages;
#endif
} pg_data_t;
</pre>

<h4 id="toc_0.5.3.2">zone</h4>
<pre>
struct zone {
	/* Fields commonly accessed by the page allocator */

	/* zone watermarks, access with *_wmark_pages(zone) macros */
	unsigned long watermark[NR_WMARK];                          // 内存域水印，通过宏 *_wmark_pages(zone) 访问

	/*
	 * When free pages are below this point, additional steps are taken
	 * when reading the number of free pages to avoid per-cpu counter
	 * drift allowing watermarks to be breached
	 */
	unsigned long percpu_drift_mark;

	/*
	 * We don't know if the memory that we're going to allocate will be freeable
	 * or/and it will be released eventually, so to avoid totally wasting several
	 * GB of ram we must reserve some of the lower zone memory (otherwise we risk
	 * to run OOM on the lower zones despite there's tons of freeable ram
	 * on the higher zones). This array is recalculated at runtime if the
	 * sysctl_lowmem_reserve_ratio sysctl changes.
	 */
	unsigned long		lowmem_reserve[MAX_NR_ZONES];           // 数组分别为各种内存域指定了若干页，用于一些无论如何都不能失败的关键性内存分配

	/*
	 * This is a per-zone reserve of pages that should not be
	 * considered dirtyable memory.
	 */
	unsigned long		dirty_balance_reserve;

#ifdef CONFIG_NUMA
	int node;
	/*
	 * zone reclaim becomes active if more unmapped pages exist.
	 */
	unsigned long		min_unmapped_pages;
	unsigned long		min_slab_pages;
#endif
	struct per_cpu_pageset __percpu *pageset;                   // 用于实现每个cpu的热/冷页帧列表
	/*
	 * free areas of different sizes
	 */
	spinlock_t		lock;
#if defined CONFIG_COMPACTION || defined CONFIG_CMA
	/* Set to true when the PG_migrate_skip bits should be cleared */
	bool			compact_blockskip_flush;

	/* pfns where compaction scanners should start */
	unsigned long		compact_cached_free_pfn;
	unsigned long		compact_cached_migrate_pfn;
#endif
#ifdef CONFIG_MEMORY_HOTPLUG
	/* see spanned/present_pages for more description */
	seqlock_t		span_seqlock;
#endif
#ifdef CONFIG_CMA
	bool			cma_alloc;
#endif
	struct free_area	free_area[MAX_ORDER];                   // 同名数据结构的数组，用于实现伙伴系统。每个数组元素都是表示某种固定长度的一些连续内存区

#ifndef CONFIG_SPARSEMEM
	/*
	 * Flags for a pageblock_nr_pages block. See pageblock-flags.h.
	 * In SPARSEMEM, this map is stored in struct mem_section
	 */
	unsigned long		*pageblock_flags;
#endif /* CONFIG_SPARSEMEM */

#ifdef CONFIG_COMPACTION
	/*
	 * On compaction failure, 1&lt;&lt;compact_defer_shift compactions
	 * are skipped before trying again. The number attempted since
	 * last failure is tracked with compact_considered.
	 */
	unsigned int		compact_considered;
	unsigned int		compact_defer_shift;
	int			compact_order_failed;
#endif

	ZONE_PADDING(_pad1_)

	/* Fields commonly accessed by the page reclaim scanner */
	spinlock_t		lru_lock;
	struct lruvec		lruvec;

	unsigned long		pages_scanned;	   /* since last reclaim */     // 指定了上次换出一页以来，有多少页未能成功扫描
	unsigned long		flags;		   /* zone flags, see below */      // 描述内存域的当前状态

	/* Zone statistics */
	atomic_long_t		vm_stat[NR_VM_ZONE_STAT_ITEMS];         // 维护了大量有关该内存域的统计信息

	/*
	 * The target ratio of ACTIVE_ANON to INACTIVE_ANON pages on
	 * this zone's LRU.  Maintained by the pageout code.
	 */
	unsigned int inactive_ratio;


	ZONE_PADDING(_pad2_)
	/* Rarely used or read-mostly fields */

	/*
	 * wait_table		-- the array holding the hash table
	 * wait_table_hash_nr_entries	-- the size of the hash table array
	 * wait_table_bits	-- wait_table_size == (1 &lt;&lt; wait_table_bits)
	 *
	 * The purpose of all these is to keep track of the people
	 * waiting for a page to become available and make them
	 * runnable again when possible. The trouble is that this
	 * consumes a lot of space, especially when so few things
	 * wait on pages at a given time. So instead of using
	 * per-page waitqueues, we use a waitqueue hash table.
	 *
	 * The bucket discipline is to sleep on the same queue when
	 * colliding and wake all in that wait queue when removing.
	 * When something wakes, it must check to be sure its page is
	 * truly available, a la thundering herd. The cost of a
	 * collision is great, but given the expected load of the
	 * table, they should be so rare as to be outweighed by the
	 * benefits from the saved space.
	 *
	 * __wait_on_page_locked() and unlock_page() in mm/filemap.c, are the
	 * primary users of these fields, and in mm/page_alloc.c
	 * free_area_init_core() performs the initialization of them.
	 */
	wait_queue_head_t	* wait_table;
	unsigned long		wait_table_hash_nr_entries;
	unsigned long		wait_table_bits;

	/*
	 * Discontig memory support fields.
	 */
	struct pglist_data	*zone_pgdat;
	/* zone_start_pfn == zone_start_paddr &gt;&gt; PAGE_SHIFT */
	unsigned long		zone_start_pfn;

	/*
	 * spanned_pages is the total pages spanned by the zone, including
	 * holes, which is calculated as:
	 * 	spanned_pages = zone_end_pfn - zone_start_pfn;
	 *
	 * present_pages is physical pages existing within the zone, which
	 * is calculated as:
	 *	present_pages = spanned_pages - absent_pages(pages in holes);
	 *
	 * managed_pages is present pages managed by the buddy system, which
	 * is calculated as (reserved_pages includes pages allocated by the
	 * bootmem allocator):
	 *	managed_pages = present_pages - reserved_pages;
	 *
	 * So present_pages may be used by memory hotplug or memory power
	 * management logic to figure out unmanaged pages by checking
	 * (present_pages - managed_pages). And managed_pages should be used
	 * by page allocator and vm scanner to calculate all kinds of watermarks
	 * and thresholds.
	 *
	 * Locking rules:
	 *
	 * zone_start_pfn and spanned_pages are protected by span_seqlock.
	 * It is a seqlock because it has to be read outside of zone-&gt;lock,
	 * and it is done in the main allocator path.  But, it is written
	 * quite infrequently.
	 *
	 * The span_seq lock is declared along with zone-&gt;lock because it is
	 * frequently read in proximity to zone-&gt;lock.  It's good to
	 * give them a chance of being in the same cacheline.
	 *
	 * Write access to present_pages and managed_pages at runtime should
	 * be protected by lock_memory_hotplug()/unlock_memory_hotplug().
	 * Any reader who can't tolerant drift of present_pages and
	 * managed_pages should hold memory hotplug lock to get a stable value.
	 */
	unsigned long		spanned_pages;
	unsigned long		present_pages;
	unsigned long		managed_pages;

	/*
	 * rarely used fields:
	 */
	const char		*name;
} ____cacheline_internodealigned_in_smp;
</pre>

<h4 id="toc_0.5.3.3">page</h4>
<pre>
kernel/include/linux/mm_types.h

/*
 * Each physical page in the system has a struct page associated with
 * it to keep track of whatever it is we are using the page for at the
 * moment. Note that we have no way to track which tasks are using
 * a page, though if it is a pagecache page, rmap structures can tell us
 * who is mapping it.
 *
 * The objects in struct page are organized in double word blocks in
 * order to allows us to use atomic double word operations on portions
 * of struct page. That is currently only used by slub but the arrangement
 * allows the use of atomic double word operations on the flags/mapping
 * and lru list pointers also.
 */
struct page {
	/* First double word block */
	unsigned long flags;		/* Atomic flags, some possibly
					 * updated asynchronously */
											 // 定义在：kernel/include/linux/page-flags.h
											 // 可以通过PageXxx来判断是否置该标志
											 // 见下面的pageflags
	struct address_space *mapping;	/* If low bit clear, points to
					 * inode address_space, or NULL.
					 * If page mapped as anonymous
					 * memory, low bit is set, and
					 * it points to anon_vma object:
					 * see PAGE_MAPPING_ANON below.
					 */
	/* Second double word */
	struct {
		union {
			pgoff_t index;		/* Our offset within mapping. */
			void *freelist;		/* slub/slob first free object */
			bool pfmemalloc;	/* If set by the page allocator,
						 * ALLOC_NO_WATERMARKS was set
						 * and the low watermark was not
						 * met implying that the system
						 * is under some pressure. The
						 * caller should try ensure
						 * this page is only used to
						 * free other pages.
						 */
		};

		union {
#if defined(CONFIG_HAVE_CMPXCHG_DOUBLE) &amp;&amp; \
	defined(CONFIG_HAVE_ALIGNED_STRUCT_PAGE)
			/* Used for cmpxchg_double in slub */
			unsigned long counters;
#else
			/*
			 * Keep _count separate from slub cmpxchg_double data.
			 * As the rest of the double word is protected by
			 * slab_lock but _count is not.
			 */
			unsigned counters;
#endif

			struct {

				union {
					/*
					 * Count of ptes mapped in
					 * mms, to show when page is
					 * mapped &amp; limit reverse map
					 * searches.
					 *
					 * Used also for tail pages
					 * refcounting instead of
					 * _count. Tail pages cannot
					 * be mapped and keeping the
					 * tail page _count zero at
					 * all times guarantees
					 * get_page_unless_zero() will
					 * never succeed on tail
					 * pages.
					 */
					atomic_t _mapcount;

					struct { /* SLUB */
						unsigned inuse:16;
						unsigned objects:15;
						unsigned frozen:1;
					};
					int units;	/* SLOB */
				};
				atomic_t _count;		/* Usage count, see below. */
			};
		};
	};

	/* Third double word block */
	union {
		struct list_head lru;	/* Pageout list, eg. active_list
					 * protected by zone-&gt;lru_lock !
					 */
		struct {		/* slub per cpu partial pages */
			struct page *next;	/* Next partial slab */
#ifdef CONFIG_64BIT
			int pages;	/* Nr of partial slabs left */
			int pobjects;	/* Approximate # of objects */
#else
			short int pages;
			short int pobjects;
#endif
		};

		struct list_head list;	/* slobs list of pages */
		struct slab *slab_page; /* slab fields */
	};

	/* Remainder is not double word aligned */
	union {
		unsigned long private;		/* Mapping-private opaque data:
					 	 * usually used for buffer_heads
						 * if PagePrivate set; used for
						 * swp_entry_t if PageSwapCache;
						 * indicates order in the buddy
						 * system if PG_buddy is set.
						 */
#if USE_SPLIT_PTLOCKS
		spinlock_t ptl;
#endif
		struct kmem_cache *slab_cache;	/* SL[AU]B: Pointer to slab */
		struct page *first_page;	/* Compound tail pages */
	};

	/*
	 * On machines where all RAM is mapped into kernel address space,
	 * we can simply calculate the virtual address. On machines with
	 * highmem some memory is mapped into kernel virtual memory
	 * dynamically, so we need a place to store that address.
	 * Note that this field could be 16 bits on x86 ... ;)
	 *
	 * Architectures with slow multiplication can define
	 * WANT_PAGE_VIRTUAL in asm/page.h
	 */
#if defined(WANT_PAGE_VIRTUAL)
	void *virtual;			/* Kernel virtual address (NULL if
					   not kmapped, ie. highmem) */
#endif /* WANT_PAGE_VIRTUAL */
#ifdef CONFIG_WANT_PAGE_DEBUG_FLAGS
	unsigned long debug_flags;	/* Use atomic bitops on this */
#endif

#ifdef CONFIG_KMEMCHECK
	/*
	 * kmemcheck wants to track the status of each byte in a page; this
	 * is a pointer to such a status block. NULL if not tracked.
	 */
	void *shadow;
#endif

#ifdef LAST_NID_NOT_IN_PAGE_FLAGS
	int _last_nid;
#endif
}
</pre>

<h4 id="toc_0.5.3.4">pageflags</h4>
<pre>
可以通过pageflag_names这个全局变量看

16/*
17 * Various page-&gt;flags bits:
18 *
19 * PG_reserved is set for special pages, which can never be swapped out. Some
20 * of them might not even exist (eg empty_bad_page)...
21 *
22 * The PG_private bitflag is set on pagecache pages if they contain filesystem
23 * specific data (which is normally at page-&gt;private). It can be used by
24 * private allocations for its own usage.
25 *
26 * During initiation of disk I/O, PG_locked is set. This bit is set before I/O
27 * and cleared when writeback _starts_ or when read _completes_. PG_writeback
28 * is set before writeback starts and cleared when it finishes.
29 *
30 * PG_locked also pins a page in pagecache, and blocks truncation of the file
31 * while it is held.
32 *
33 * page_waitqueue(page) is a wait queue of all tasks waiting for the page
34 * to become unlocked.
35 *
36 * PG_uptodate tells whether the page's contents is valid.  When a read
37 * completes, the page becomes uptodate, unless a disk I/O error happened.
38 *
39 * PG_referenced, PG_reclaim are used for page reclaim for anonymous and
40 * file-backed pagecache (see mm/vmscan.c).
41 *
42 * PG_error is set to indicate that an I/O error occurred on this page.
43 *
44 * PG_arch_1 is an architecture specific page state bit.  The generic code
45 * guarantees that this bit is cleared for a page when it first is entered into
46 * the page cache.
47 *
48 * PG_highmem pages are not permanently mapped into the kernel virtual address
49 * space, they need to be kmapped separately for doing IO on the pages.  The
50 * struct page (these bits with information) are always mapped into kernel
51 * address space...
52 *
53 * PG_hwpoison indicates that a page got corrupted in hardware and contains
54 * data with incorrect ECC bits that triggered a machine check. Accessing is
55 * not safe since it may cause another machine check. Don't touch!
56 */
57
58/*
59 * Don't use the *_dontuse flags.  Use the macros.  Otherwise you'll break
60 * locked- and dirty-page accounting.
61 *
62 * The page flags field is split into two parts, the main flags area
63 * which extends from the low bits upwards, and the fields area which
64 * extends from the high bits downwards.
65 *
66 *  | FIELD | ... | FLAGS |
67 *  N-1           ^       0
68 *               (NR_PAGEFLAGS)
69 *
70 * The fields area is reserved for fields mapping zone, node (for NUMA) and
71 * SPARSEMEM section (for variants of SPARSEMEM that require section ids like
72 * SPARSEMEM_EXTREME with !SPARSEMEM_VMEMMAP).
73 */
74enum pageflags {
75	PG_locked,		/* Page is locked. Don't touch. */                  // 页被锁定。例如，在磁盘I/O操作中涉及的页
76	PG_error,                                                           // 在传输页时发生I/O错误
77	PG_referenced,                                                      // 刚刚访问过的页
78	PG_uptodate,                                                        // 在完成读操作后置位，除非发生磁盘I/O错误
79	PG_dirty,                                                           // 页已经被修改
80	PG_lru,                                                             // 页在活动或非活动页链表中
81	PG_active,                                                          // 页在活动页链表中
82	PG_slab,                                                            // 包含在slab中的页框
83	PG_owner_priv_1,	/* Owner use. If pagecache, fs may use*/
84	PG_arch_1,
85	PG_reserved,                                                        // 页框留给内核代码或没有使用
86	PG_private,		/* If pagecache, has fs-private data */             // 页描述符的private字段存放了有意义的数据
87	PG_private_2,		/* If pagecache, has fs aux data */
88	PG_writeback,		/* Page is under writeback */                   // 正在使用writepage方法将页写到磁盘上
89#ifdef CONFIG_PAGEFLAGS_EXTENDED
90	PG_head,		/* A head page */
91	PG_tail,		/* A tail page */
92#else
93	PG_compound,		/* A compound page */                           // 通过扩展分页机制处理页框
94#endif
95	PG_swapcache,		/* Swap page: swp_entry_t in private */         // 页属于对换高速缓存
96	PG_mappedtodisk,	/* Has blocks allocated on-disk */              // 页框中的所有数据对应于磁盘上分配的块
97	PG_reclaim,		/* To be reclaimed asap */                          // 为回收内存对页已经做了写入磁盘的标记
98	PG_swapbacked,		/* Page is backed by RAM/swap */
99	PG_unevictable,		/* Page is "unevictable"  */
100#ifdef CONFIG_MMU
101	PG_mlocked,		/* Page is vma mlocked */
102#endif
103#ifdef CONFIG_ARCH_USES_PG_UNCACHED
104	PG_uncached,		/* Page has been mapped as uncached */
105#endif
106#ifdef CONFIG_MEMORY_FAILURE
107	PG_hwpoison,		/* hardware poisoned page. Don't touch */
108#endif
109#ifdef CONFIG_TRANSPARENT_HUGEPAGE
110	PG_compound_lock,
111#endif
112	__NR_PAGEFLAGS,
113
114	/* Filesystems */
115	PG_checked = PG_owner_priv_1,
116
117	/* Two page bits are conscripted by FS-Cache to maintain local caching
118	 * state.  These bits are set on pages belonging to the netfs's inodes
119	 * when those inodes are being locally cached.
120	 */
121	PG_fscache = PG_private_2,	/* page backed by cache */
122
123	/* XEN */
124	PG_pinned = PG_owner_priv_1,
125	PG_savepinned = PG_dirty,
126
127	/* SLOB */
128	PG_slob_free = PG_private,
129};
</pre>

<h4 id="toc_0.5.3.5">kmem_cache</h4>
<pre>
kernel/include/linux/slub_def.h

/*
 * Slab cache management.
 */
struct kmem_cache {
    struct kmem_cache_cpu __percpu *cpu_slab;
    /* Used for retriving partial slabs etc */
    unsigned long flags;                                                               // cache属性的描述标识
    unsigned long min_partial;
    int size;       /* The size of an object including meta data */                    // 分配给对象的内存大小，可能大于实际对象的大小
    int object_size;    /* The size of an object without meta data */                  // 对象的实际大小
    int offset;     /* Free pointer offset. */
    int cpu_partial;    /* Number of per cpu partial objects to keep around */
    struct kmem_cache_order_objects oo;                                                // 存放分配给slab的页框的阶数(高16位)和slab中的对象数量(低16位)

    /* Allocation and freeing of slabs */
    struct kmem_cache_order_objects max;
    struct kmem_cache_order_objects min;
    gfp_t allocflags;   /* gfp flags to use on each alloc */
    int refcount;       /* Refcount for slab cache destroy */
    void (*ctor)(void *);
    int inuse;      /* Offset to metadata */
    int align;      /* Alignment */
    int reserved;       /* Reserved bytes at the end of slabs */
    const char *name;   /* Name (only for display!) */                                  // slab cache名称
    struct list_head list;  /* List of slab caches */                                   // kmem_cache链表
#ifdef CONFIG_SYSFS
    struct kobject kobj;    /* For sysfs */
#endif
#ifdef CONFIG_MEMCG_KMEM
    struct memcg_cache_params *memcg_params;
    int max_attr_size; /* for propagation, maximum size of a stored attr */
#endif

#ifdef CONFIG_NUMA
    /*
     * Defragmentation by allocating from a remote node.
     */
    int remote_node_defrag_ratio;
#endif
    struct kmem_cache_node *node[MAX_NUMNODES];
};
</pre>

<h4 id="toc_0.5.3.6">kmem_cache_cpu</h4>
<pre>
kernel/include/linux/slub_def.h

struct kmem_cache_cpu {
    void **freelist;    /* Pointer to next available object */
    unsigned long tid;  /* Globally unique transaction id */
    struct page *page;  /* The slab from which we are allocating */
    struct page *partial;   /* Partially allocated frozen slabs */
#ifdef CONFIG_SLUB_STATS
    unsigned stat[NR_SLUB_STAT_ITEMS];
#endif
};
</pre>

<h4 id="toc_0.5.3.7">kmem_cache_node</h4>
<pre>
kernel/mm/slab.h

/*
 * The slab lists for all objects.
 */
struct kmem_cache_node {
    spinlock_t list_lock;

#ifdef CONFIG_SLAB
    struct list_head slabs_partial; /* partial list first, better asm code */
    struct list_head slabs_full;
    struct list_head slabs_free;
    unsigned long free_objects;
    unsigned int free_limit;
    unsigned int colour_next;   /* Per-node cache coloring */
    struct array_cache *shared; /* shared per node */
    struct array_cache **alien; /* on other nodes */
    unsigned long next_reap;    /* updated without locking */
    int free_touched;       /* updated without locking */
#endif

#ifdef CONFIG_SLUB
    unsigned long nr_partial;                                                           // partial个数
    struct list_head partial;                                                           // partial链表头，它通过page-&gt;lru组成一个链表。可以通过v.v &amp;((struct page*)0)-&gt;lru看lru相对于page的偏移
#ifdef CONFIG_SLUB_DEBUG
    atomic_long_t nr_slabs;
    atomic_long_t total_objects;
    struct list_head full;
#endif
#endif
};
</pre>

<h4 id="toc_0.5.3.8">vmap_area/vmap_area_list</h4>
<pre>
kernel/mm/vmalloc.c

LIST_HEAD(vmap_area_list);
它是vmap_area.list的链表
40struct vmap_area {
41	unsigned long va_start;
42	unsigned long va_end;
43	unsigned long flags;
44	struct rb_node rb_node;         /* address sorted rbtree */
45	struct list_head list;          /* address sorted list */
46	struct list_head purge_list;    /* "lazy purge" list */
47	struct vm_struct *vm;
48	struct rcu_head rcu_head;
49};
</pre>

<h4 id="toc_0.5.3.9">vm_struct</h4>
<pre>
29struct vm_struct {
30	struct vm_struct	*next;
31	void			*addr;
32	unsigned long		size;
33	unsigned long		flags;
34	struct page		**pages;
35	unsigned int		nr_pages;
36	phys_addr_t		phys_addr;
37	const void		*caller;
38};
</pre>

<h2 id="toc_0.6">调试经验</h2>
<h3 id="toc_0.6.1">全局变量</h3>
<table>
<tr>
<td>
<strong>变量名</strong>
</td>
<td>
<strong>作用</strong>
</td>
</tr>
<tr>
<td>
high_memory
</td>
<td>
直接映射的物理地址末尾对应的线性地址保存在high_memory变量中
</td>
</tr>
<tr>
<td>
mem_map
</td>
<td>
struct page，所有物理内存页，可以通过((vir_add-vir_offset)&gt;&gt;12)来定位kernel线性地址所在页</BR>数组中每个元素和物理内存页面一一对应,整个数组就代表着系统中的全部物理页面。</BR>如系统中有76G物理内存,则物理内存页面数为76*1024*1024k/4K= 19922944个页面,mem_map[ ]数组大小19922944
</td>
</tr>
<tr>
<td>
slab_caches
</td>
<td>
kmem_cache链表头
</td>
</tr>
<tr>
<td>
kmem_cache
</td>
<td>
存kmem_cache对象
</td>
</tr>
<tr>
<td>
kmalloc_caches
</td>
<td>
&nbsp;
</td>
</tr>
<tr>
<td>
swapper_pg_dir
</td>
<td>
内核全局页目录，地址为0xc0004000~0xc0008000
</td>
</tr>
<tr>
<td>
init_mm
</td>
<td>
主内存描述符
</td>
</tr>
<tr>
<td>
min_low_pfn/max_low_pfn/max_pfn
</td>
<td>
pfn数
</td>
</tr>
</table>

<h3 id="toc_0.6.2">文件节点</h3>
<ul>
<li>
/d/memblock/reserve

<li>
/proc/meminfo

<li>
/proc/slabinfo

<li>
/proc/buddyinfo

</ul>

<h3 id="toc_0.6.3">内存泄漏检测</h3>
<pre>
内核配置：
CONFIG_DEBUG_KMEMLEAK=y
CONFIG_DEBUG_KMEMLEAK_EARLY_LOG_SIZE=1000

其中 CONFIG_DEBUG_KMEMLEAK_EARLY_LOG_SIZE 的大小跟board 的kernel porting相关，
有的不需要定义，有的需要定义大一点，可以在kmemleak.c 中模块初始化代码中调试.

kmemleak 模块初始化成功后，会产生/sys/kernel/debug/kmemleak 这个文件

操作命令如下:
#su
#echo scan &gt; /sys/kernel/debug/kmemleak 扫描泄漏
#cat /sys/kernel/debug/kmemleak   查看泄漏
#echo clear &gt; /sys/kernel/debug/kmemleak 清除结果
 
当出现泄漏后，会有提示，比如
unreferenced object 0xd25f3cc0 (size 64):
  comm "Binder_5", pid 1257, jiffies 68676 (age 3105.280s)
  hex dump (first 32 bytes):
    00 00 00 00 01 00 00 00 00 00 00 00 00 00 00 00  ................
    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
  backtrace:
    [&lt;c00fa860&gt;] create_object+0x12c/0x248
    [&lt;c0540fd4&gt;] kmemleak_alloc+0x88/0xcc
    [&lt;c00f6f10&gt;] kmem_cache_alloc_trace+0x13c/0x1f4
    [&lt;c026749c&gt;] ion_carveout_heap_map_dma+0x34/0xcc
    [&lt;c0265280&gt;] ion_alloc+0x170/0x3f0
    [&lt;c02655c0&gt;] ion_ioctl+0xc0/0x410
    [&lt;c010d9b0&gt;] do_vfs_ioctl+0x4f4/0x568
    [&lt;c010da6c&gt;] sys_ioctl+0x48/0x6c
    [&lt;c000f800&gt;] ret_fast_syscall+0x0/0x48
    [&lt;ffffffff&gt;] 0xffffffff
</pre>

<h3 id="toc_0.6.4">内存越界检测</h3>
<pre>
内核配置为使用 slub 作为内存分配器 ,slub 本身提供了检查越界的接口，如果 kernel 刚启动就要检查内存破坏，则需要编译的时候配置 CONFIG_SLUB_DEBUG_ON=y
否则可以使用 slabinfo –d A 来打开检查功能，打开后， slub 会在内存后面加一些关键字， 释放的时候会检查是否被破坏，如果破坏了，check_bytes_and_report中print一个警告,
可以修改check_bytes_and_report后面部分的代码，在debug版本中加入panic让系统死机来报告内存越界错误。
 
static int check_bytes_and_report(struct kmem_cache *s, struct page *page,
            u8 *object, char *what,
            u8 *start, unsigned int value, unsigned int bytes)
{
    u8 *fault;
    u8 *end;
    fault = memchr_inv(start, value, bytes);
    if (!fault)
        return 1;
    end = start + bytes;
    while (end &gt; fault &amp;&amp; end[-1] == value)
        end--;
    slab_bug(s, "%s overwritten", what);
    printk(KERN_WARN "INFO: 0x%p-0x%p. First byte 0x%x instead of 0x%x\n",
                    fault, end - 1, fault[0], value);
    print_trailer(s, page, object);
    restore_bytes(s, what, value, fault, end);
    return 0;
}
</pre>

<h3 id="toc_0.6.5">Native 的内存泄漏和越界</h3>
<pre>
Android的native内存泄漏可以用valgrind工具，但是这个工具在android里运行太慢，Android里的bionic库提供了内存泄漏的检测方法
见 bionic/libc/bionic/malloc_debug_common.c 里的注释.
所有的native 内存分配函数都在libc库里，为了跟踪，需要使用这个库的特别版版本libc_malloc_debug_leak.so和libc_malloc_debug_qemu.so,可以看看eng或这user-debug版中的/system/lib/下是否有这两个文件，其中libc_malloc_debug_qemu.so是模拟器用的。
 
在 malloc_debug_common.c 中的内存调试靠读取属性 libc.debug.malloc 来控制的，属性值含义如下:
libc.debug.malloc      1    检测内存泄漏
libc.debug.malloc      5    分配的内存用0xeb 填充，释放的内存用0xef填充
libc.debug.malloc     10   内存分配打pre- 和post- 的桩子，可以检测内存的overruns
libc.debug.malloc     20   SDK 模拟器上检测内存用
 
简单的内存泄漏检测可以通过adb shell 写为
# setprop libc.debug.malloc  1
#stop
#start 
然后logcat 就可以显示内存泄漏，有的server可能stop杀不掉，为了开机就启动，可以在开机的时候就设置属性。
可以修改init.rc, 增加 setprop libc.debug.malloc  10 属性 编译烧机，
或者
#adb remount
#vi system/build.prop 增加 libc.debug.malloc = 10 属性
</pre>

<h3 id="toc_0.6.6">根据已分配内存地址找到slab</h3>
<p>
<img src="file:///home/likewise-open/SPREADTRUM/hua.fang/vimwiki/vimwiki/images/mem_find_page.PNG" />
</p>

<h3 id="toc_0.6.7">找到所有partial的页</h3>
<pre>
从slab_caches中得到kmem_cache
kmem_cache-&gt;list是串起kmem_cache的链表，所以从slab_caches中得到链表值，就可以通过偏移得到kmem_cache
v.v &amp;((struct kmem_cache*)0)-&gt;list得到list相对于kmem_cache的偏移
v.v (struct kmem_cache*)(0xABCDEFGH - 0xXY)
</pre>
<p>
<img src="file:///home/likewise-open/SPREADTRUM/hua.fang/vimwiki/vimwiki/images/mem_partital_list.PNG" />
</p>

<h3 id="toc_0.6.8">查找kmem里的freelist</h3>
<pre>
kmem_cacheslab有两种管理方式：
1、cpu_slab
  cpu级别，这类slab一般是刚申请和释放的slab，系统认为有可能还在cache里
  它是per cpu的，通过数据结构kmem_cache_cpu来管理
    成员变量freelist管理free的内存
    成员partial管理下一个slab，它是page数据结构，pages表示这是第几个page，而next则指向下一个page
2、node
  普通的slab
  它指向了page数据结构，里面的freelist构建出free的内存，而list.prev则构建出partial的列表
</pre>
<p>
<img src="file:///home/likewise-open/SPREADTRUM/hua.fang/vimwiki/vimwiki/images/slab_freelist.PNG" />
</p>

<h3 id="toc_0.6.9">虚拟地址转物理地址</h3>
<pre>
每个进程都有自己的页目录，放在task_struct-&gt;mm.pgd
高1G的地址空间给内核用，各进程间是共享的，用全局变量swapper_pg_dir(0xc0004000~0xc0008000)来保存信息，进程在fork时，会把这部分内容copy过去

通过页目录项的低2位，可以看用的是哪种映射方式：
00：无效
10：Section Base，页目录项的高12位，加上虚拟地址的低20位，就得到物理地址，内核的低地址区就是这们映射的
01：Coarse page table Base，页目录项保存着页表的基址，然后通过页表偏移，得到页表值。根据页表项的最后2位，又分为大页（16K）和小页（4K）。我们使用的是小页管理
11：Fine page table，每个页大小为1K
</pre>
<p>
<img src="file:///home/likewise-open/SPREADTRUM/hua.fang/vimwiki/vimwiki/images/vaddr_to_paddr.PNG" />
</p>

<h3 id="toc_0.6.10">计算总内存大小</h3>
<pre>
contig_page_data.node_spanned_pages是总的page数量
memory_size=contig_page_data.node_spanned_pages*4K
</pre>

<hr />
<h2 id="toc_0.7">参考资料</h2>
<p>
<a href="http://www.cnblogs.com/zhaoyl/p/3695517.html">Linux内存管理原理</a></BR>
<a href="http://www.cnblogs.com/wang_yb/archive/2013/05/23/3095907.html">《Linux内核设计与实现》读书笔记（十二）- 内存管理</a></BR>
<a href="http://blog.csdn.net/crazyjiang/article/details/7903772">linux kernel内存映射实例分析</a></BR>
<a href="http://ilinuxkernel.com/?p=1013">Linux内核高端内存</a></BR>
<a href="http://ilinuxkernel.com/?cat=4">内存管理</a></BR>
<a href="http://tomhibolu.iteye.com/blog/1214892">浅析linux内核内存管理之kmalloc</a></BR>
<a href="http://blog.csdn.net/hanchaoman/article/details/6942138">kernel hacker修炼之道之内存管理-高端内存(上)</a></BR>
<a href="http://blog.csdn.net/hanchaoman/article/details/6942140">kernel hacker修炼之道之内存管理-高端内存(下)</a></BR>
</p>

<p>
--
</p>

<p>
<a href="http://blog.csdn.net/vanbreaker/article/details/7694648">Linux Slub分配器</a></BR>
<a href="http://blog.chinaunix.net/uid-7588746-id-259800.html">Linux内存 之 Slub分配器</a></BR>
<a href="http://www.ibm.com/developerworks/cn/linux/l-cn-slub/">Linux SLUB 分配器详解</a></BR>
</p>

<p>
--
</p>

<p>
<a href="http://lli_njupt.0fees.net/ar01s12.html">页表机制</a></BR>
<a href="http://blog.csdn.net/xiaojsj111/article/details/11065717">ARM MMU框架</a></BR>
<a href="http://blog.csdn.net/cosmoslhf/article/details/42742975">arm的2级页表在Linux内核创建过程解析</a></BR>
<a href="http://blog.csdn.net/hsly_support/article/details/7463609">浅析linux内核内存管理之分页</a></BR>
<a href="http://www.360doc.com/content/13/1225/14/10366845_339992189.shtml">Android/Linux Kernel 記憶體管理-入門筆記</a><BR>
</p>

<p>
--
<a href="http://www.tuicool.com/articles/eYj6ji">Android 内存检测</a><BR>
</p>

</body>
</html>
